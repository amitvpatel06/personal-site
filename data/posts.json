[
	{
		"id":3,
		"title":"Modeling Twitter Sentiment",
		"subtitle":"The models only took three days to run!",
		"author":"Amit",
				"content": "<p>As mentioned in an earlier post, I've been working on a research project for using deep learning algorithms to classify the sentiment for Twitter data. Warning: this post assumes some familiarity with machine learning! In this post, I'll be analyzing some of my results. To get started, here's a side by side comparison of the models' performance. </p> | <img src='img/sidebysidevalid.jpg'> | <br> | <img src='img/sidebysidepercent.jpg'> | <p> Much to my suprise, the best performing model(and by a significant margin) was the LSTM RNN. I think perhaps adding extra layers and a bidirectional component(essentially 2 RNNs working in tandem) allowed the model to better learn details. Of the three, it had by far the most paramters. The combination of the sequential nature of the model and its powerful memory cells allowed it to outperform the CNN. Looking back on the experiments, I would probably bump up the number and size of filters on the CNN, lower the dropout rate on all the models, and increase the number of words I fed the model. Since all the tweet sentences are of variable length, I had to pick a standard size to feed the CNN so that computations could be parallelized. In order to provide a challenge for the models, I decided to give a relatively short window of 10 words to each model.</p> | <br> | Next, let's look at the training graphs of each model: | <img src='img/RNN.jpg'> | <br> |  <img src='img/CNN.jpg'> | <br> |  <img src='img/LSTM.jpg'> | <br> | <p> In all 3 graphs, we see that surpisingly enough, the validation loss is lower than the training loss. This can be explained with a couple details of how I trained my models. First, I graph the average epoch batch loss and the training epoch is run before each validation epoch. Since the training set improves as it runs, it overreports its loss relative to that of the validation set. Additionally, I implemented a feature called dropout regularization for each training epoch. Dropout(visualized in the post theme picture) automatically randomly ignores a certain portion of a network's hidden units(it essenitally sets them to 0). This prevents overfitting by preventing hidden units from 'coadapting' as the model learns from the training data. One of the big problems I ran into while performing these experiments was runtime: running each model for all 30 epochs took a whole day on a regular AWS free tier server. In order to get the results in a reasonable amount of time, I had to use a large GPU instance. One thing I'm looking into with a new project is actually finding ways to train my models without renting a GPU server by farming computation out to worker nodes. You can check it out on my github under CrowdLearn! I'll soon be putting up a posts with more implementation details.",
		"img": "img/dropout.png", 
		"date": "8/1/16"
	},
	{
		"id":2,
		"title":"My CS 224D Follow Along",
		"subtitle":"I wish Penn had a deep learning class",
		"author":"Amit",
		"content": "<p> One of the projects I undertook this summer was doing a follow-along of Stanford's CS 224d class. The lectures are posted on youtube(although they are now private and you have to check the subreddit to find them), and all the assignments are posted on the website. The course covers the application of deep learning algorithms to Natural Language Processing. In this post, I'd like to share some general tips I've learned about creating and using models effectively(it assumes some familiriaty with machine learning). I'll be going over some of the implementation steps taught in the class and that I applied to my own project, where I used deep learning for the task of sentiment analysis on tweets.</p> | <h1>Choose your problem and architecture carefully!</h1> | <p>Deep learning for NLP has come a long way in the last few years. Every year, new tasks are being cracked by ever-better models and new performance records are being set on old tasks. Selecting the right model is critical for performance. Model selection can sometimes be counterintuitive and referring to literature is usually the best way to get started. For my models, I decided to use a simple Recurrent Neural Network without anything fancy added in as a baseline, a bidirectional RNN with LSTM memory cells and extra layers, and a Convolutional Neural Network from Yoon Kim's <a href='https://arxiv.org/abs/1408.5882'>2014 paper</a>. RNNs are more intuitively fit for the task of reading text because they operate on sentences from left to right, however, it can be shown they are actually special cases of CNNs, which act by operating on different subsets of the input sentence using convolution functions. </p> | <h1> Step 2: Parameter Initialization and Optimization Tricks</h1> | <p>When you are writing your model, how you initialize your model's parameters and how you optimize can yield significant improvements in performance. Be sure to check literature both on your architecture and your specific task to see if the initialization and optimization engine you are using(using vanilla batch stochastic gradient descent is usually a pretty bad idea!) are optimal. I went with xavier weight initialization and the AdamOptimizer. Additionally, especially with neural networks that have thousands of parameters, a regularization term is usually necessary. First, train your model and check how far your model's performance on its training examples diverges from its performance on the cross validation data. If the gap is large, this usually indicates a need for regularization! A lot of these model decisions(like learning rate, regularization rate, number of layers, etc.) are called hyperparameters. At larger scales, often algorithms like Bayesian Hyperparameter Optimization are used to fine tune performance.</p> <h1>Build your model for scale</h1> <p> One of the most important things about deep learning and one of the things that I am most interested in about it is scale. Deep learning has become incredibly effective as the amount of available training data has scaled up. Building a dataset and a model that take advantage of that is key, especially with more ambitious projects. I used an AMI I found with tensorflow and its corresponding CUDA drivers so that I could take advantage of GPU processing. I managed to scrounge about $250 of free AWS credit(they give out a lot of free credit if you look hard enough!) so that I was able to run my models(which compared to production grade ones are kind of small) on a powerful server box. Those looking to scale up even more can looking into asynchronous training on multiple machines and using distributed frameworks like Apache Spark.",
		"img": "img/neuralnet.jpg", 
		"date": "7/20/16"
	},
	{
		"id":1,
		"title":"Creating this website!",
		"subtitle":"Isomorphic what??",
		"author":"Amit",
		"content": "<p> Welcome to my website! I'll be posting here from time to time with updates on what I've been up to and projects I've been working on.</p> | <p> As a relatively new developer, making this website was an adventure. It is one of the first major projects I've undertaken completely on my own. Being forced to design and implement my stack from end to end ended up being a lot of fun and teaching me a lot. </p> | <p> First, a little bit about my stack. For the backend, I decided to go with a Node server with Express and MongoDB, having used both before in other projects and finding them easy to use while still having a lot of functionality. For the front end of my website I decided to use my favorite library from my internship: React. React makes front end programming easy and very modular. Pretty much every piece of HTML on this site that is reusable is a React component! React also makes updating components and managing dependencies easy and intuitive. React is usually used with a client side architecture called Flux(together it's known as Redux) that allows programmers to avoid creating complicated two-way data dependencies that end up causing more bugs than they fix. Additionally, React is compatible with another key feature I wanted to implement with my website: server side rendering.</p> | <h1> Server Side Rendering and Isomorphic Javascript</h1> | <p> Server side rendering is perhaps best explained with pictures: </p> | <img src='img/tradmvc.jpg'> <p> In a traditional client-side Javascript setting, the client-side code handles much of the logic, including  rendering the page, managing interactions with users, storing and modelling data, and talking to external APIs. This allows a separation of concerns between the client code and the server code(meaning they can easily be written in different languages), but comes with several issues. Pages that render using client-side Javascript have poor SEO since web crawlers often don't execute Javascript before indexing(workarounds exist but involve some difficulty). Additionally, since the server doesn't do the work of rendering the page, performance often suffers because users have to wait for client-side code to finish loading and then render, which has a drastic effect on loading times. Loading times(especially for consumer facing businesses) are critical for user acquisition and retention. Companies have found user retention can suffer up to 7% for every extra second it takes a page to load, and Google itself recommends a load time of one second or less for a page. Finally, having a divided codebase can make traditional client-side rendering applications difficult to maintain and grow.</p>  | <img src='img/universal.jpg'> | <p>One solution that addresses all three of these issues is called an isomorphic application: the client and server are written together in the same language and the server renders the page before sending it to the user. This allows a single, unified application that is easier to maintain, is indexed by search engines fully, and provides faster loading times! Since the server renders the page, rendered pages can also be cached and repeated computation can be avoided. I used Express and ReactDOMServer to do server side rendering in my application. The application is deployed on EC2 and uses nginx to handle serving the page.</p> <p>That's it for this post! In the future I'll be posting about other projects I've been working on.</p>",
		"img": "img/server.jpg", 
		"date": "7/6/16"
	}
]